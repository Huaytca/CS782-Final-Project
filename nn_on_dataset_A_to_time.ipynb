{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolating the Transient to Speed up Training Time and hopefully Accuracy Too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import hilbert\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sktime.transformations.panel.rocket import MiniRocketMultivariateVariable\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from module import my_improved_bayesian_change_point_detection, load_data, preprocess_bluetooth_signals\n",
    "from rf_classifier import RFClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the root directory\n",
    "root_directory = os.path.join(os.path.join(os.getcwd(), 'Bluetooth Datasets'), 'Dataset 5 Gsps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = load_data(root_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Section: Preprocess data by removing spur signals, normalizing, applying hilbert transform, and isolating transient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing 2548 signals from dataset A...\n",
      "Processing signal 1/2548...\n",
      "Processing signal 1001/2548...\n",
      "Processing signal 2001/2548...\n",
      "Preprocessing complete!\n"
     ]
    }
   ],
   "source": [
    "preprocessed_data = preprocess_bluetooth_signals(data, signal_column='signal', dataset='A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data['transient'] = None\n",
    "for idx, row in preprocessed_data.iterrows():\n",
    "    analytic_signal = row['analytic_signal']\n",
    "    start_idx, end_idx, _, _ = my_improved_bayesian_change_point_detection(analytic_signal, window_size=600, overlap=0.65, start_threshold=10, end_threshold=2)\n",
    "    preprocessed_data.at[idx, 'transient'] = analytic_signal[start_idx:end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_transient_features(transient, fs):\n",
    "    \"\"\"Extract 9 HOS features + duration from a transient signal\"\"\"\n",
    "    # Convert pandas Series to numpy array\n",
    "    analytic = transient\n",
    "    \n",
    "    # Compute analytic signal\n",
    "    # analytic = hilbert(signal)\n",
    "    \n",
    "    # Instantaneous characteristics\n",
    "    amplitude = np.abs(analytic)\n",
    "    amplitude_centered = amplitude - np.mean(amplitude)\n",
    "\n",
    "    phase = np.unwrap(np.angle(analytic))\n",
    "    time = 1 / fs\n",
    "    mu_f = np.mean(np.diff(phase)/(2 * np.pi))  # Mean frequency\n",
    "    phase_nonlinear = phase - 2 * np.pi * mu_f * time\n",
    "    phase_centered = phase_nonlinear - np.mean(phase_nonlinear)\n",
    "\n",
    "    frequency = np.diff(phase)/(2*np.pi)  # Handle length mismatch\n",
    "    frequency_centered = frequency - np.mean(frequency)\n",
    "    \n",
    "    # Calculate HOS features\n",
    "    features = {\n",
    "        'amp_var': np.var(amplitude_centered),\n",
    "        'amp_skew': skew(amplitude_centered, bias=False),\n",
    "        'amp_kurt': kurtosis(amplitude_centered, fisher=True, bias=False),\n",
    "        'phase_var': np.var(phase_centered),\n",
    "        'phase_skew': skew(phase_centered, bias=False),\n",
    "        'phase_kurt': kurtosis(phase_centered, fisher=True, bias=False),\n",
    "        'freq_var': np.var(frequency_centered) if len(frequency) > 0 else 0,\n",
    "        'freq_skew': skew(frequency_centered, bias=False) if len(frequency) > 0 else 0,\n",
    "        'freq_kurt': kurtosis(frequency_centered, fisher=True, bias=False) if len(frequency) > 0 else 0,\n",
    "        'duration': len(analytic)\n",
    "    }\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and center features\n",
    "def normalize_features(df):\n",
    "    \"\"\"Z-score normalization for all feature columns\"\"\"\n",
    "    normalized_df = df.copy()\n",
    "    for col in normalized_df.columns:\n",
    "        if col != 'duration':\n",
    "            mean = normalized_df[col].mean()\n",
    "            std = normalized_df[col].std()\n",
    "            normalized_df[col] = (normalized_df[col] - mean)/std\n",
    "    return normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for all transients\n",
    "# feature_df = preprocessed_data['transient'].apply(extract_transient_features).apply(pd.Series)\n",
    "feature_df = preprocessed_data['transient'].apply(\n",
    "    lambda x: extract_transient_features(x, fs=5e9)\n",
    ").apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply normalization (excluding duration initially)\n",
    "normalized_features = normalize_features(feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize duration separately\n",
    "duration_mean = feature_df['duration'].mean()\n",
    "duration_std = feature_df['duration'].std()\n",
    "normalized_features['duration'] = (feature_df['duration'] - duration_mean)/duration_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_features = normalized_features.copy()\n",
    "\n",
    "# Clip features between -3 and 3 (z-score units)\n",
    "for col in clipped_features.columns:\n",
    "    clipped_features[col] = np.clip(clipped_features[col], -3.5, 3.5)\n",
    "\n",
    "# Add normalized features back to original dataframe\n",
    "# complete_df = pd.concat([preprocessed_data, normalized_features], axis=1)\n",
    "complete_df = pd.concat([preprocessed_data, clipped_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7c16e8ea4610>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, test_size=0.2, random_state=42):\n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    df['label_encoded'] = le.fit_transform(df['label'])\n",
    "    \n",
    "    # Extract features and labels\n",
    "    feature_cols = [col for col in df.columns if col not in ['label', 'label_encoded']]\n",
    "    X = df[feature_cols].values\n",
    "    y = df['label_encoded'].values\n",
    "    \n",
    "    # Stratified split (120 train, 30 test per class)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=test_size, \n",
    "        stratify=y,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Normalize features\n",
    "    # scaler = StandardScaler()\n",
    "    # X_train = scaler.fit_transform(X_train)\n",
    "    # X_test = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_test, y_test, epochs=2500, lr=0.0001):\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.long)\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    # Initialize model, loss, optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr ,weight_decay=0.001)\n",
    "    scheduler = StepLR(optimizer, step_size=500, gamma=0.9)\n",
    "    \n",
    "    # Training loop\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    test_dataset = TensorDataset(\n",
    "        torch.tensor(X_test, dtype=torch.float32),\n",
    "        torch.tensor(y_test, dtype=torch.long)\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    final_acc = 100 * correct / total\n",
    "    print(f'Final Test Accuracy: {final_acc:.1f}%')\n",
    "    return final_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFClassifier(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=128, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Dropout(p=0.4, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): Tanh()\n",
      "    (5): Dropout(p=0.4, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): Tanh()\n",
      "    (8): Dropout(p=0.4, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=17, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31136/2895676867.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['label_encoded'] = le.fit_transform(df['label'])\n"
     ]
    }
   ],
   "source": [
    "nn_df = complete_df[['label', 'amp_var',\n",
    "       'amp_skew', 'amp_kurt', 'phase_var', 'phase_skew', 'phase_kurt',\n",
    "       'freq_var', 'freq_skew', 'freq_kurt', 'duration']]\n",
    "\n",
    "# Prepare data\n",
    "X_train, X_test, y_train, y_test, classes = prepare_data(nn_df, random_state=42)\n",
    "\n",
    "# Initialize model\n",
    "model = RFClassifier(input_size=X_train.shape[1], hidden_size=128, num_classes=len(classes), dropout_rate=0.4) # .to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 78.4%\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(model, X_train, y_train, X_test, y_test, epochs=2500, lr=1e-3)\n",
    "end_time = time.perf_counter()\n",
    "final_acc = evaluate_model(trained_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 339.333641 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f'Execution time: {end_time - start_time:.6f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs782_final_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
